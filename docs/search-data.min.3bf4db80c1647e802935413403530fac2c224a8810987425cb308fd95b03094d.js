(function(){const pages=[{"idx":0,"href":"/posts/machine-learning-for-the-arts/a-glance-of-ai/","title":"A Glance of AI","content":" A Glance of AI How to say Hi in Klingonese?  In this section a task and corresponding AI system will be designed.\n Klingonese is an artificial alien language, every word of which is well documented and can be easily looked up by Star Trek fans. But when we\u0026rsquo;re faced with real Aliens, in like 100 years, we\u0026rsquo;ll have to listen to languages we\u0026rsquo;ve never heard of before. We\u0026rsquo;ll know nothing about the vocabulary, grammar, or structure, and we\u0026rsquo;ll have no family or root languages to compare to. In this case, we could use AI as a fast learner.\nThere\u0026rsquo;ll be three stages:\n Learn\nAI could learn a language in a very different way. It doesn\u0026rsquo;t need to care about memory and searching. While we learn by words and grammars, AI can learn it as a whole through scenario-based conversations: we would have AI observe our friendly neighbor aliens and record their everyday life and communication (with their consent). Then AI would pare specific scenarios and the sentences. After a large amount of training, it would know about the language from common daily usage, to professional phrases.\n Documentation\nThen AI will learn about how we document current languages: how dictionaries are formed, how to write example sentences, and how to build records, etc. AI will help us document the alien language in a way human can easily understand and lookup.\n Teach\nFinally, AI will learn how current languages are taught, as design textbooks and class materials accordingly. And we\u0026rsquo;ll get access to those languages we could never understand before.\n  Actually, while alien language seems too imaginary, a lot of languages on this planet are endangered due to lack of documentation and proper sufficient usage. We could also use the process and system above as a remedy.\nSummary  Inputs: Recordings of conversations, articles, documents in the target language. Outputs: The language explain in known languages. Training Data: Need to be collected. Ethical Considerations: Since it\u0026rsquo;ll be a full-time recording, the privacy has to be the first thing to be considered. Data will be all made anonymous. The participants will be able to decide when to stop the recording and can exclude any piece of recording out of the database.  A People\u0026rsquo;s Guide to AI Page 23-28 (Everyday AI Activity)/36-41 (Embodying Social Algorithms).\n\rScreenshots\r‚Üï\r\r\r\r\r"},{"idx":1,"href":"/posts/machine-learning-for-the-arts/imagenet/","title":"ImageNet","content":" ImageNet The first thing that amazed me was that the project of ImageNet took full advantage of WordNet and benefited from Mechanical Turk. In this way, the data is so organized by synonym sets and, sounds crazy, human-annotated. The data can be looked up easily through treemap visualization.\nWhile each syntex, as claimed, has more than 1000 images on average to be illustrated, I still found some weak portions. With only 138 picture for People section, a lot of subdivisions, including business people and lobby boy, have no photos. The same is for Misc section. The main reason, as far as I\u0026rsquo;m concerned, is privacy. In ImageNet\u0026rsquo;s Introduction page, a whole section started with Does ImageNet own the images? Can I download the images? explains one, all photos here in the ImageNet are originally public available, and two, ImageNet only provides links and thumbnails to the actual photos for most people, like a search engine. However, there might still be potential privacy and copyright concerns. Although all the pictures are uploaded to the public space, most people might not be prepare to have their photos been used for research purposes, and been classified to be search and view easily. Given the fact that most people never read those user agreements, their consent to the photos been reused or shared, when they uploaded the photos to this services, is also doubted. ImageNet seems to have found a \u0026ldquo;perfect\u0026rdquo; way to avoid this concern coming into being, and intentionally gave up the magic power in some sensitive sections.\nTo many debating over whether citizens should sacrifice their privacy to support technology development have taken place around. Privacy is a major part of our social identity thus plays an important role of our modern humanity. Yet there\u0026rsquo;s no clue that technology and humanity are naturally opposite. We see lots of technology boosted the formation of humanity we define today, and many of them were particularly developed to protect our privacy and identity. One possible reason might be that we haven\u0026rsquo;t found the most proper way to do AI. Back to the solution itself, we shall all agree that the current solution is working and descent, yet unsatisfying. Are we only capable to analyze pixel and color? And will never understand how the world is observed and the view of it is established? I doubt that.\nTry-on Sessions Cropped screenshots of webcam image classification. Fist line (1-5): Glass Cup, Closet, Boxes, Battery, DSLR Lens. Second line (6-10): Bird on Screen, Board, Mouse, Sunglasses, Pen.\nI tried several things with a focus on the following scenarios:\n Transparent (1) Group of things (2-3) Hard for human (4-5) On screen (6) With people (7-9) Tiny target (10)  Results    Original Prediction     Glass Cup beaker, paintbrush   Closet wardrobe, closet, press   Boxes comics   Battery revolver, six-gun, six-shooter   DSLR Lens hand blower, blow dryer, hair dryer   Bird on Screen laptop, laptop computer   Board remote control   Mouse cellular telephone, mouse   Sunglasses sunglasses, dark glasses, shades   Pen syringe    "},{"idx":2,"href":"/posts/communications-lab/one-story/","title":"One Story","content":" One Story \u0026ndash; Reflection on Chimamanda Ngozi Adichie‚Äôs TED talk [1].\nMaking one story the only story might be dangerous, but inevitable.\nIt\u0026rsquo;s true that each press, media, or storyteller might have their own perspectives, but it\u0026rsquo;s also true that they are all under certain meta narratives [2]. Social media have granted us countless resources of information, but we still feel the cyberspace bias. We could imagine looking into a mirror broken into pieces reflecting the world around, the reflection will be different but not much if all of them are on one flat floor. It\u0026rsquo;s also true for stories. Meta narrative can be our culture, history, political standpoints, fairy tales told from childhood, Shakespeare\u0026rsquo;s drama, etc. All these built up individual narrative systems and as a consequence, constrain our stories into a pool.\nAnother stakeholder involved in story telling cannot be ignored as well: It\u0026rsquo;s also about what we choose to believe as listeners. Most people just unconsciously believe everything they read or hear. Most of the rest only believe things they want to believe, with bias and prejudice. It\u0026rsquo;s hard to tell which circumstance is worse, cause either of which are more or less falling into the trap of meta narrative.\nIn my opinion, it\u0026rsquo;s fine to have just one story, as it\u0026rsquo;s how most of our world is running on. More importantly, we can have a story that is embracing more than one point of view and allowing critic. Netflix\u0026rsquo;s recent documentary American Factory is a good example. What makes a single story dangerous is not that there\u0026rsquo;s not a second one, but the very only one is manipulated based on the storyteller\u0026rsquo;s interests. If so, it\u0026rsquo;s useless to have more stories since we\u0026rsquo;re just jumping from one lie to another. Or it could be even more dangerous as people are comforted to have more fake options. One story could have multiple perspectives, and we just choose not to.\n Is there free will? Is telling partial truth lying?  Reference  Chimamanda Ngozi Adichie: The danger of a single story Eric X. Li: A tale of two political systems  "},{"idx":3,"href":"/posts/","title":"Posts","content":""},{"idx":4,"href":"/posts/performative-avatars/i-avatar/","title":"I, Avatar","content":" I, Avatar Hundreds of Avatar making softwares are there for tuning our digital representations as the expectations. Specialized softwares like Adobe Fuse and MakeHuman; Sculpting softwares like Cinema4D and blender; Social media like Facebook and twitter; Mobile apps like Memoji and Kapu\u0026hellip;\nHere I used Fuse (beta) and Kapu, a newly released app by Tencent (the largest game company on the planet), created two very distinguishable Avatars of me, and analyzed the differences. While both of them are 3D software, Fuse is more for 1:1 human body making, and Kapu is more like a canvas for preschool animation.\nKAPU As the visual character making iOS app for the instant message tool QQ (targeting younger generations), Kapu is social oriented, infantilized, and silly in some perspectives. It\u0026rsquo;s not for any serious sculpting, but minimizing the making process by preset designs and limited adjustable features. Authenticity is the least to be considered in this ACG world. But honestly, the process is quite playful and pleasing, with several features worth mentioning:\n\n  \n Maniac/Interactive Motions  Every time the user switch back to the character page, (it)\u0026rsquo;ll maniacally wave Hi to you with exaggerate gestures. Unlike traditional static Avatar making software, the whole process is more like an interactive game instead of a tedious sculpting work with the embedded motion features. While waving seems childish, corresponding feedback after the user changes a particular part helps highlight the adjustment and strengthen the \u0026ldquo;gaming\u0026rdquo; experience. For example, a blink after choosing an eye type.\n     On-body Adjustment  The most important thing for a mobile Avatar app targeting kids is usability: understandable and usable for anyone who has no modeling experience. Putting clear control points on the body, instead of using sliders, Kapu is very easy and straightforward to use. This feature is also quite similar to the Liquify section in Photoshop CC.\n   Avatar in the House  As a beginner level Avatar creator, the app doesn\u0026rsquo;t offer many parameters to be changed, and the whole process is heavily based on picking from preset haircut, facial features and clothing - can be purchased in the online market. The market is also serving for another feature of the app: My Space. The Avatar can not only stand on the grey plain floor, but also be realtime rendered into a room that can be decorated and visited by the user\u0026rsquo;s friends.\n  Social Avatars Avatars are never apart from a society as humans can never. And I can see Kapu\u0026rsquo;s attempt to build digital citizenship for younger users - and once the group formed, they shall never leave. When those physical characteristics will not only be seen by the users themselves but also their QQ friends and even anonymous people, users start to hesitate, and choices disobey their interests. Tencent\u0026rsquo;s using anime and infantilized to make it easy: You can never go wrong with a cute smiling character. Actually, the community culture of QQ, started 20 years ago in 1999, plays an important role in the anonymity trend of China\u0026rsquo;s Internet, with which as the mainstream communication tool, people got used to not use self photos as profile photo, and use made-up names instead of real names.\nFuse Beta \n  \nIn contrast, Fuse is a very serious tool: detailed and cooperated with Adobe suite and Mixamo for further productions. While it\u0026rsquo;s also based on picking from some very limited preset templets, it also offers a great many of sliders for users to customize the face, body, and texture. The user can then choose from another very limited library of ugly clothes.\nYour browser does not support the video tag.\r\rAlthough there\u0026rsquo;re tons of parameters for users to adjust, templets as the base are all western faces. And I spent 30 minutes to just get it a little closer to my Asian looking.\nIdeas Quick notes of suggestions for Avatar making software.\n Aging:The models from Fuse look too mature for me, and adjusting the dimensions or length of eyes and nose manually is tedious. Since we\u0026rsquo;ve already have age recognizing models that could tell an approximate number based on a photo, is it possible for a Machine Learning based model help us generally adjust facial features? And compile the whole process into a slider: from 0 to 99 years old.  Questions  Is people\u0026rsquo;s affection to Avatars affected by how alike between them or not?\nWe see very human-alike Avatars, like the characters in SimCity, with preserved body ratio and facial features; some others are beautified intentionally, including the one mentioned above; while there\u0026rsquo;re also some quite different ones: monsters, elves, or even the cursor. What actually decides whether an Avatar will build a emotional connection with (its) owner? Should it be alike a real human? Or (it)\u0026rsquo;s owner more specifically? What role does this similarity play?\n What makes an Avatar, an Avatar?\nThere\u0026rsquo;re more and more Avatar artists, workers, servers, etc. Those are Avatars without a specified owner. Are they still an Avatar? Can we call any artificial character an Avatar? Is the Avatar of AI an Avatar?\n  Related Readings  Virtual Worlds Are Real Touching online funerals that gamers hold for friends they have never met Controlling vs ‚ÄúBeing‚Äù Your Video Game Avatar When We Play Video Games, Who Are We? Aspects of the Self (Ch.7 of Life on the Screen (1995))  "},{"idx":5,"href":"/docs/","title":"Docs","content":""},{"idx":6,"href":"/","title":"Home","content":" JPL is Ê±üÊ≤õÂ∂∫. Ê±üÊ≤õÂ∂∫ is Peiling Jiang. This is the blog of Peiling Jiang\u0026rsquo;s works in the program of Interactive Media Arts.\n Search by tags: IMA / Fall19  Search by categories: Reading / Project  üôÜ‚Äç‚ôÇÔ∏è More information will be updated soon.\n"}];window.bookSearch={pages:pages,idx:lunr(function(){this.ref("idx");this.field("title");this.field("content");pages.forEach(this.add,this);}),}})();