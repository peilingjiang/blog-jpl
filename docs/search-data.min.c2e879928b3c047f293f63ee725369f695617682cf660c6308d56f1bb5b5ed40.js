(function(){const pages=[{"idx":0,"href":"/posts/performative-avatars/mirror/","title":"Mirror","content":"I experimented basic sequencer and related video making features with Mirror, a one-minute short clip. During the process, material making, planar reflections, blending animations, and Unreal Engine sequencer were tired, broken, fixed, broken again, fixed again, and finally helped me make the video possible.\nWith former experience of Premiere and DaVinci, I would say the sequencer part is not the hardest to understand. However, the material of mirror, or any kinds of reflective material, really took me hours to make. And the final outcome is still not what was in my mind before I got my hands on the software.\nThe blueprint script of the material, though, is more than simple. However, just to understand the planar reflection is not an object that can be directly rendered in the video, but a “guide” for reflective materials to capture the surroundings took me more than an hour, with several unusable clips rendered. Then a laundry list of parameters to be adjusted took another hour.\n  "},{"idx":1,"href":"/posts/","title":"Posts","content":""},{"idx":2,"href":"/posts/machine-learning-for-the-arts/dancing-glow/","title":"Dancing Glow","content":" Your browser does not support the video tag.\r\rThis week I tried real-time data collection and multiple, different types of inputs and output with the new Neural Network feature of ml5.js. Dancing Glow, as a result, is a visual project in which lines following the body will change to different colors according to body positions as trained. Here\u0026rsquo;s the LINK to the p5 web editor.\nTrain Data Collection I set up and used two models in this project - one is 'regression' model that trains the program to predict the color of any given poses; another one is 'classification' model that trains the program to distinguish normal poses to emit pose, when posing which the glowing colors will emit to the surroundings. To collect data for both models, the user need to add data with either a color array as target output, or a label of \u0026quot;Emit\u0026quot; or \u0026quot;Not Emit\u0026quot;. Moreover, to collect data more efficiently, while data is collected to color regression model, the same pose information is also passed into the classification model as \u0026ldquo;Not Emit\u0026rdquo; label.\n// Add a training example function addExample_1() { // Add to brain_1 as color input  // Add to brain_2 as Not Emit category  if (poses.length \u0026gt; 0) { let poseInput = get_body_data(poses); let target = bodyColor; // Add data for regression training  // Input 26, Output 3  brain_1.addData(poseInput, [target[0], target[1], target[2]]); // Add data for categorization training  // Input 26, Output 1  brain_2.addData(poseInput, [\u0026#34;Not Emit\u0026#34;]); } } function addExample_2() { // Add to brain_2 as Emit category  if (poses.length \u0026gt; 0) { let poseInput = get_body_data(poses); brain_2.addData(poseInput, [\u0026#34;Emit\u0026#34;]); } }  Mouth Switch Another interesting thing, which you might have noticed, is that I open and close my mouth as the switch of data collection. We all know about the 3..2..1.. countdown, which is decent and boring. Here instead I use face-api.js (a built-in library of ml5.js) to track the points of my mouth and calculate the distances of several signature points to get to know whether I\u0026rsquo;m opening my mouth or not, thus if to collect the pose data.\nposeNet = ml5.poseNet(video, modelReady); // This sets up an event that fills the global variable \u0026quot;poses\u0026quot; // with an array every time new poses are detected poseNet.on(\u0026quot;pose\u0026quot;, function (results) { poses = results; }); // face-api.js // No need to create faceBrain here const faceOptions = { withLandmarks: true, withExpressions: false, withDescriptors: false }; faceStatusLabel = 2; faceapi = ml5.faceApi(video, faceOptions, faceReady);  Here poseNet is for tracking poses as inputs for machine learning, and faceAPI is solely for mouth opening and closing. The problem is that face-api.js is really heavy and slows down the frame rate significantly once it\u0026rsquo;s running. To save computing resources, instead of letting it recursively call faceapi.detect(gotFaces), I move it to draw() and will only be executed when the user choose a color or emit class to collect data.\nClassification However, the classification model is not working, getting weird predictions in which one label\u0026rsquo;s confidence is always 1 and another one\u0026rsquo;s is undefined.\nArray(2) 0: {label: \u0026quot;Emit\u0026quot;, confidence: 1} 1: {label: \u0026quot;Not Emit\u0026quot;, confidence: undefined} tensor: t {kept: false, isDisposedInternal: false, shape: Array(2), dtype: \u0026quot;float32\u0026quot;, size: 1, …} length: 2 __proto__: Array(0)  Interface The interface for user to collect data and train the model is another focus of the project. It\u0026rsquo;s designed for users to easily select data target (for color regression or emit classification), adjust color, add more classes of data (user can add up to 8 colors), and get informed of the status behind the scene.\nAt the top of the interface are add color class button for adding more color classes, add data for emit class button, add data for color class button, and train! button. Since the project involves multiple library and the training of more than one model, I added a section of status feedback for users to know about the progress more easily. Below the video are status reports of the loading of libraries and training of the model - grey means not start yet, yellow means in-progress, and green means finished.\nFeature Works  Fix the bug of 'classification' model. As mentioned before, the classification brain is not working and currently I don\u0026rsquo;t know if it\u0026rsquo;s a bug from ml5 or my own code.\n A better output visualization, and interface design.\n  "},{"idx":3,"href":"/posts/machine-learning-for-the-arts/housing-price/","title":"Housing Price","content":"  This page is a composite of Assignment 4a and 4b.\n The Dataset After a long time wandering in Kaggle and OpenML, I finally decided to choose the dataset of Housing price in Beijing (Housing price of Beijing from 2011 to 2017, fetching from Lianjia.com.) from Kaggle as the data I\u0026rsquo;m to collect, wrangle and build on for this week\u0026rsquo;s assignment.\nMy interests and curiosity of housing price might be arisen by Ekene Ijeoma\u0026rsquo;s Wage Islands: Immigrants project - a sculpture to visualize where low-wage immigrant workers can afford to rent in NYC.\nFile The original dataset is formatted in .csv file with 318852 rows (data points) and 26 columns (variables). According to the author, Qichen Qiu, it includes URL, ID, Lng, Lat, Community ID, Trade Time, DOM (days on market), Followers, Total Price, Price, Square, Living Room Num, Drawing Room Num, Kitchen Num, Bathroom Num, Building Type (1-4), Construction time, Renovation Condition (1-4), Building Structure (1-6), Ladder Ratio, Elevator, Property Rights for Five Years, Subway, District, Community Average Price.\nI was confused by some variables at first since they are only displayed as a number instead of a clear definition. And the author elaborates some of them as below:\n Lng: and Lat coordinates, using the BD09 protocol. Basically the location. buildingType: Tower (1), bungalow (2)，combination of plate and tower (3), plate (4). renovationCondition: Other (1), rough (2), Simplicity (3), hardcover (4). buildingStructure: Unknown (1), mixed (2), brick and wood (3), brick and concrete (4), steel (5) and steel-concrete composite (6). ladderRatio: the proportion between number of residents on the same floor and number of elevator of ladder. It describes how many ladders a resident has on average. elevator: Have (1), not have (0). fiveYearsProperty: (1) if the owner have the property for less than 5 years. It\u0026rsquo;s related to China\u0026rsquo;s housing purchase policy. district: Dongcheng (1), Fengtai (2), Tongzhou (3), Daxin (4), Fangshan (5), Changping (6), Chaoyang (7), Haidian (8), Shijinshan (9), Xicheng (10), Pinggu (11), Mentougou (12), Shunyi (13).  All the data are last updated a year ago (2018) and reflect the housing prices from 2011 to 2017. Since there\u0026rsquo;s a URL linked to China\u0026rsquo;s largest house trading platform, I assume all these data were captured from that website.\nOne good thing about this dataset worth mentioning is that it provides different accuracy of some variables. For example, floor. Except recording the exact floor number of the house, the dataset also use 低 (Low), 中 (Middle), 高 (High) to help simplify the classification of this particular variable. And if our prediction is not every floor sensitive, we can just use the blurred data for the training. However, the Chinese characters also cause some problems.\nWhy? The first thing I noticed is that there are a great many of variables that could affect the price of a house. And this data can be used to learn how the housing price in Beijing is affected by construction time or the number of bathroom. Moreover, since the dataset is collected cumulatively through 7 years, it could also be used to decode the pattern of how house price is changing growing, and predict how it will change in the future.\nProblems With Lng and Lat, and district number, the location is kind of clearly reflected in the data - In terms of a city like Beijing, location of the property might be the most important thing related to the price. However, 3 administrative districts are missing in this data collection. Also, due to the source of the data is a Chinese website, there\u0026rsquo;s a lot of errors and garbled text in original data and I have to set encoding to GB2312 to have those text correctly displayed, while still with a lot of error.\nWhen a data is missing, sometimes it says \u0026ldquo;nan\u0026rdquo;, while others might say \u0026ldquo;未知\u0026rdquo; (unknown). I think this could also potentially confuse the machine during the training. The documentation of the dataset is also not very good, with some important description missing that I had to search in the discussion section.\nClean I tried several ways to prepare a cleaner dataset for the further use.\n I split floor, which consists of both a character indicating the height level of the house, and the exact floor number, into two different columns: floor class and floor num. And replaced 底低中高顶 - Bottom, Low, Middle, High, Top with 1, 2, 3, 4, and 5.\n I replaced all missing data, whether it says \u0026ldquo;nan\u0026rdquo; or \u0026ldquo;unknown\u0026rdquo; with 0.\n I deleted URL, ID, Community ID which wouldn\u0026rsquo;t affect housing prices.\n  A link to cleaned small dataset is here (NYU login required).\nThe Neural Network Then I tried an in-progress feature of ml5.js: ml5.neuralNetwork().\nTitanic Dataset First, I played with examples given in class - a model based on Titanic Survival Dataset, with the following steps:\n Clean the raw data and replace all missing data. Delete unrelated data including name, sibsp, parch, etc. Try different ways of data representation:  Use \u0026ldquo;lived\u0026rdquo;/\u0026ldquo;died\u0026rdquo; instead of 0/1 for survived parameter; Use \u0026ldquo;first\u0026rdquo;/\u0026ldquo;second\u0026rdquo;/\u0026ldquo;third\u0026rdquo; instead of 1/2/3 for pclass (passenger class) parameter.   Variable Types When fed into p5 web editor, the model can still be trained and predict as before. While the outcomes are different: when using numbers as labels, all the output are numbers and need to be translated by an extra piece of code to be more readable. Surprisingly, string-based labels also help the model work better with a smaller loss: (Epochs = 20)\n   survived pclass Average loss     Number Number 0.531   Number String 0.530   String Number 0.528   String String 0.485    (Losses vary between sessions and numbers above are averages of 3 trials.)\n Q: Why loss would vary between different sessions?\n Then another question came into my mind: we were asked to clean the data by making all floating numbers integers, will this also affect the performance of the model training and the prediction? I looked into this by changing fare values between raw floating numbers and rounded integers. And here\u0026rsquo;s the results: (Epochs = 20)\n   fare Number loss_1 loss_2 loss_3     Integer 0.528 0.529 0.529   Float 0.528 0.529 0.528    As a result, integers don\u0026rsquo;t really help the training perform better. However, all results above might only be true for the very specific algorithm that ml5.neuralNetwork is using.\nIdle Columns To further test the feature, I added an extra column that will not be used by the model nor mentioned in inputs in the code, and the training couldn\u0026rsquo;t finish with a TypeError always occurred after Epoch 19 (the last Epoch but one).\nTypeError: Cannot convert undefined or null to object  Hyperparameters All the experiments above are based on default settings of ml5.neuralNetwork. But we can also modify those based on our interests.\nlet nnOptions = { activationHidden: \u0026#39;sigmoid\u0026#39;, // \u0026#39;relu\u0026#39;, \u0026#39;tanh\u0026#39;  learningRate: 0.25, hiddenUnits: 16, modelLoss: \u0026#39;categoricalCrossentropy\u0026#39;, // \u0026#39;meanSquaredError\u0026#39; }; let trainingOptions = { epochs: 32, batchSize: 64 };  activationHidden  Reference: Activation Functions in Neural Networks\n Training Performance when using Sigmoid, ReLu, and tanH\nWhile the default activation function is $\\sigma$ (sigmoid), which ranges from 0 to 1 - perfect when need to predict something with a confidence, there\u0026rsquo;re also many other functions to choose from.\n ReLu: range $[0, \\infty)$  $$f(x)=\\cases{ 0 \u0026amp;x $\\lt$ 0\\cr x \u0026amp;x $\\geqslant$ 0}$$\nThe training became much faster when I set activationHidden: 'relu' with a smaller loss of around 0.44. (While the loss of Sigmoid-based training is around 0.49.)\n tanH: range $(-1, 1)$  $$f(x)=tanh(x)=\\frac{2}{1+e^{-2}}-1$$\nI feel the training is even faster with tanH as activation function. The loss is around 0.47.\nlearningRate Learning rate, together with hidden units and batch size, should affect the performance of each time the learning algorithm working through the entire training dataset. And the more epochs the training has, the more opportunities the training dataset will have to update the internal model parameters thus will have better results. (Epochs = 15, Size of Training Set = 1308)\n   Test learningRate batchSize hiddenUnits loss_1 loss_2 loss_3 Average $\\sigma$     lR1 0.25 64 16 0.491 0.493 0.493 0.4923 0.00115   lR2 0.0001 64 16 0.783 0.666 0.762 0.737 0.06238   lR3 1 64 16 0.474 0.480 0.482 0.4787 0.00416   lR4 5 64 16 0.514 5.519 0.586 2.2063 2.86908   bS1 0.25 1 * 16 0.442 0.444 0.445 0.4437 0.00153   bS2 0.25 128 ** 16 0.502 0.509 0.500 0.5037 0.00473   bS3 0.25 1308 *** 16 0.617 0.626 0.629 0.624 0.00624   hU1 0.25 64 2 0.500 0.498 0.523 0.507 0.01389   hU2 0.25 64 64 0.523 0.522 0.537 0.5273 0.00839   hU3 0.25 64 1000 5.519 5.519 5.599 7.2123 2.93294    * Stochastic Gradient Descent - Takes much longer time for each epoch to complete. (Bathes per epoch = 1308)\n** Mini-Batch Gradient Descent (Bathes per epoch = 10)\n*** Batch Gradient Descent - Very fast. (Bathes per epoch = 1)\nAs a result, the default settings (lR1) of ml5.neuralNetwork has the most stable training output. Also, smaller batch sizes will help improve the output loss by trading training time.\nepochs Intuitively, the more epochs the training model has, the better the output will be. And it was proved through the experiments recorded below. (Use default settings for learning rate, batch size and hidden units.)\n   epochs loss_1 loss_2 loss_3 Average $\\sigma$     1 0.628 0.644 0.654 0.642 0.01311   5 0.535 0.537 0.531 0.5343 0.00305   10 0.500 0.505 0.503 0.5027 0.00251   20 0.486 0.487 0.489 0.4873 0.00153   50 0.475 0.475 0.472 0.474 0.00173   100 0.463 0.456 0.453 0.4573 0.00513    modelLoss  Reference:\nUsage of loss functions - Keras Documentation\nWhy You Should Use Cross-Entropy Error \u0026hellip;\nWhy is the Cross Entropy method preferred over Mean Squared Error?\n According to the reference, Cross-Entropy Error is preferred for classification (this case), while Mean Squared Error is one of the best choices for regression. In the previous sections, I played with the labels of the same meanings while in different data types: string or integer. And would it be affected by modelLoss parameter with 'categoricalCrossentropy' or 'meanSquaredError'? (Epochs = 15)\n   modelLoss survived/pclass/sex loss_1 loss_2 loss_3 Average $\\sigma$     categorical Number 0.495 0.493 0.499 0.4957 0.00306   meanSquared Number 0.493 0.493 0.495 0.4937 0.00115   categorical String 0.495 0.492 0.497 0.4947 0.00252   meanSquared String 0.489 0.496 0.495 0.4933 0.00379    We can see that either model loss type doesn\u0026rsquo;t really make a huge difference in terms of average performance here, and the advantage of either error model isn\u0026rsquo;t shown through the data. However, Cross-Entropy Error makes string data perform more stable and Mean Squared Error makes number data perform more stable.\nHouse Price in Beijing Then I tried to train my own model with ml5.neuralNetwork(). The data is much more complex with more columns of parameters and more rows of datasets.\nWrangling According to the results from Titanic exercise, I made following changes to my dataset from last data wrangling practice (Link to the final version for training. (NYU login required)):\n Delete all idle columns and make it as simple as possible. Output: totalPrice. Inputs: tradeTime, square, livingRoom, kitchen, bathRoom, floorClass, constructionTime, elevator, fiveYearsProperty, subway, district, and communityAverage.\nlet nnOptions = { modelLoss: \u0026#39;meanSquaredError\u0026#39;, dataUrl: \u0026#39;house_price_clean_small_2.csv\u0026#39;, inputs: [\u0026#39;tradeTime\u0026#39;, \u0026#39;square\u0026#39;, \u0026#39;livingRoom\u0026#39;, \u0026#39;kitchen\u0026#39;, \u0026#39;bathRoom\u0026#39;, \u0026#39;floorClass\u0026#39;, \u0026#39;constructionTime\u0026#39;, \u0026#39;elevator\u0026#39;, \u0026#39;fiveYearsProperty\u0026#39;, \u0026#39;subway\u0026#39;, \u0026#39;district\u0026#39;, \u0026#39;communityAverage\u0026#39;], outputs: [\u0026#39;totalPrice\u0026#39;], task: \u0026#39;regression\u0026#39;, // Different from the Titanic example  debug: true };  Since it\u0026rsquo;ll be a linear prediction - the price number of a house, I\u0026rsquo;ll use \u0026quot;meanSquaredError\u0026quot; for the modelLoss type. Thus all data should be transferred to numerical type. For example, 1 for Yes, and 0 for No. However, for district number, which doesn\u0026rsquo;t represent the housing prices in a linear way (Houses in District 1 are not necessarily cheaper than those in District 7), so I still keep it as strings of names of the districts.\n Dates are all formatted as numbers without any punctuation. For example, Aug. 9th, 2016 are formatted as 20160809.\n Instead of using 0 for missing data in the former practice, I choose to use the method introduced in Titanic tutorial - to calculate the average and then pick random numbers within one standard deviation of the average for the missing data.\n  Here\u0026rsquo;s the first rows of the final .csv file:\ntotalPrice,tradeTime,square,livingRoom,kitchen,bathRoom,floorClass,constructionTime,elevator,fiveYearsProperty,subway,district,communityAverage 415,20160809,131,2,1,1,4,2005,1,0,1,Chaoyang,56021 575,20160728,132.38,2,1,2,4,2004,1,1,0,Chaoyang,71539 1030,20161211,198,3,1,3,3,2005,1,0,0,Chaoyang,48160 297.5,20160930,134,3,1,1,1,2008,1,0,0,Changping,51238 392,20160828,81,2,1,1,3,1960,0,1,1,Dongcheng,62588 Training Fortunately, although several annoying red alerts popped at first, none of them were unreasonable and I was able to solve them quickly. The biggest problems came out of the difference between the task of \u0026quot;classification\u0026quot; and \u0026quot;regression\u0026quot;. First, I need to look up the preview version of ml5.js, and figured out \u0026ldquo;regression\u0026rdquo; is the task label that I would use. And when gotResults(), I need to print out results.outputs.value instead of results[0].label.\nfunction gotResults(err, results) { if (err) { console.error(err); } else { console.log(results); select(\u0026#39;#result\u0026#39;).html(`prediction: ${results.outputs.value.toFixed(3)}`); } }  Here\u0026rsquo;s the link to the p5 web editor of the project.\nResults The model seems working well for most of times, and sometimes making reasonable predictions for test inputs. It\u0026rsquo;s performance, however, is quite unstable. Sometimes the learning curve (loss) drops dramatically at the beginning, while sometimes it stays the same for the whole time.\nLearning Curves of 50/20/30/30 epochs.\nAnd they are all making very different predictions. The top two (50 and 20 epochs) predict the price to be 570 and 470, while the bottom two (30 epochs) predict the price to be 810 and 27, which is the most outrageous one.\nThen I tried to set learningRate = 0.05 for smaller changes during each epoch, and hiddenUnits = 32, and the performance and predictions became much more stable. The final prediction of the price of the virtual house mentioned above should be around 700-800.\nHowever, the model also acts very weirdly sometimes.\nAs far as I\u0026rsquo;m concerned, it wouldn\u0026rsquo;t work when I tried to upload and feed larger datasets. I tried with a very small dataset of 2000 rows (compared to original 318852 rows) and the model could run. However, when I tried with even 4000 or 5000 rows, the model started to fail and outputted NaN for loss, while still \u0026ldquo;unconsciously\u0026rdquo; ran to the last epoch and started predicting - NaNs.\n Epoch: 0 - loss: NaN Epoch: 1 - loss: NaN Epoch: 2 - loss: NaN Epoch: 3 - loss: NaN ...  Ideas  In addition to documenting your exercise, reflect on your experience using a new feature of an open source library. One of the big challenges of designing a higher level library for machine learning is finding the right balance between hiding the lower level details and allowing for flexibility / customization. Thinking about the difference between programming all the elements of the model and working with the ml5.neuralNetwork() function, how did this balance sit with you? Is the code easy for you to follow? Do you understand what the library is doing? What are some roadblocks you anticipate for using the library with your own data? Do you have any ideas for improvements or modifications to the ml5.js library?\n With a programming background of python, I used to include many smaller, more specific libraries in the code, unlike larger, and one-for-all libraries like p5.js and ml5.js. For me, to make the library easier to use, with more built-in capabilities, must also sacrifice the customizability and flexibility. For example, I have no idea how ml5.neuralNetwork can be trained with an article or a group of pictures, instead of .json or .csv files. This might also because of the lack of documentation at this stage. ml5.neuralNetwork() help us a lot by hiding inner components carefully, and we only need to adjust Hyperparameters that would directly affect the training performance: activation function, learning rate, hidden units, and error type, instead of starting with programming a perceptron.\nCurrently, all the training need to be done during the setup(), and would it be possible to have users interactively input data for training from the p5 canvas, and start training afterwards?\n"},{"idx":4,"href":"/posts/performative-avatars/wrap/","title":"Wrap","content":" Remesh Now it\u0026rsquo;s time to wrap the scanned model to eliminate broken or undetailed parts.\nUsing Wrap3 and a base mesh, I re-meshed my scanned model and made it closed, less polygon and control points, and detailed in terms of hands and face. However, a few things didn\u0026rsquo;t go very well.\n Feet. I was scanned with my shoes on while the base model has bare feet. What would happen (and happened) is that if I didn\u0026rsquo;t select foot parts as isolated polygon (and the scanned model would be more important during morphing), the feet and fingers would turn into strange squeezed shape with separated tips still there. Alternatively, if I selected feet, the front part of it, I would get a half-foot-half-shoe model, with black fabric texture and a Nike logo on it. Finally, I chose to have a logo on my feet.\n Eye. My eyes became weirdly big after the morph. It\u0026rsquo;s probably because I just added morphing alignment points on eye corners but not eye lashes.\n Clothes. All clothes are mixed with my body, blended to each other. And gradient colors are like those in Adobe Fuse.\n  Animate Then I used Adobe Mixamo to add skeleton and animation to the model.\nI chose many different ones including Capoeira, Stop jumping, Sitting (with shaking legs), Illegal elbow punch, and Strafe. Mixamo skeleton works very well with the remeshed model with even just 8 markers (chin, wrists, elbows, knees, and groin) for auto-rigging.\nQuestions  Is our gender selection in digital sphere the true reflection of our self-identity?\nWe might think that we intend to create or choose digital Avatars that can realize our ideal self, which could better represent our self-identities. However, according to this survey, only a portion of players claimed that their Avatars are idealized version of themselves. Could other reasons affect our choices of our own Avatars?\n We cannot deny that the identity (and life, body or face) of most internet famous people are objectified and sold on the internet. But what\u0026rsquo;s the moment that the objectification happen？ Is it when they bake their posts based on advertisement requirements? Or is it when they worried about fans and the public reactions? If exhaustedly caring about public criticism a part of objectification, are we all objectifying ourselves when we post - what we basically just bake for people to see and appreciate - on social media?\n  Related Readings Avatars \u0026amp; Gender\n Men Are Working Out Their Issues By Playing As Their Lovers and Exes in RPGs How Video Game Breasts Are Made Dealing With Harassment in VR How Video Game Avatars Can Help Trans Kids Cope  "},{"idx":5,"href":"/posts/machine-learning-for-the-arts/violinist/","title":"Violinist","content":"  UPDATE: The name of the project has been changed to Guitarist!\n Group Project with Heather Kim\nViolinist enables user to play violin (or potentially any type of instrument) by just posing as they are playing it. This simple have-a-try project is based on p5.js, ml5.js, and poseNet, and help us understand the basic use of poseNet and p5.serialControl. Here\u0026rsquo;s the LINK to p5 web app (p5.serialControl and Arduino required).\nposeNet() in p5.js The goal is to have the webcam and p5 web app be able to distinguish the relaxing, prepared, and playing status with poseNet(). And a few points we noticed when tackling the challenge are as follows.\nEliminate Noise Raw poseNet() output is quite noisy with more than 10 digits after the decimal point and the value changing 60 times a second. As a result, the keypoints drawn are shaking all the time and unusable for us as our calculation will be based on the position of those dots. Thus we set noiseDelta = 6, and if the horizontal or vertical position change between frames are smaller than the value, it will be regarded as noise. Moreover, we use the same strategy for minimizing the noise of angle changing, by setting sensitivity = 12 (degrees).\nWhile we setup all these rules above, poseNet will just return unreasonable points - jumping around - sometimes when the background is complex or the skeleton is covered by accessories, like a watch, or itself. Thus we added an offTime counter and the pose will only be regarded as not playing when some keypoint is off more than 100 frames.\nCorrect Pose PoseNet() is good at abstracting the skeleton and keypoints from the body, but we didn\u0026rsquo;t find a direct way to label specific poses (e.g. running, playing violin) based on it - which might require another machine learning library. And we don\u0026rsquo;t want the speaker to beep once the bow angle changes. We then set a series of standard semi-transparent key points to help user match their pose to the performing pose. Once the pose matched, the hint areas will dim. However, this is also the constrain of the project, and if playPose() is based on the label or name of the pose, it would be much better.\nMirror Effects All the VIDEO assets drawn in p5.js sketch are flipped horizontally, unlike a natural mirror. These feature caused a huge problem for us since it\u0026rsquo;s really confusing when the figure on the screen acts the opposite way of you and you need to match your pose to the preset one. We first flip the displayed video while this will not affect the data poseNet is reading.\npush(); // Move image by the width of image to the left translate(video.width, 0); // Then scale it by -1 in the x-axis to flip the image scale(-1, 1); image(video, 0, 0, width, height); pop();  Then we setup a Point class and store all keypoints into one array. When drawing the points, renderer will only get data from getX() and getY(), and the calculation and rendering will be separated.\nclass Point { constructor(x, y, ind, part) { this.x = x; this.y = y; this.px = x; this.py = y; ... } ... getX() { // Return mirrored x return (width - this.x); } getY() { return this.y; } ... }  Physical Computing We want the buzzer to beep at different rates as the user play faster or slower. To do that, we map the velocity of angle changing to delay(x) - the faster the user is playing, the bigger the vel, and the smaller the value of x. (Please turn down the volume before playing the video.)\n\nYour browser does not support the video tag.\r\rLink to source code files.\nFuture Works  Due to time and the scale of the project, we didn\u0026rsquo;t have enough time to further tuning the tone of buzzer. But it\u0026rsquo;ll be the first to-do on the list. Moreover, instead of playing just one note, a sequence of notes can be programmed to beep to create a real song. And mp3 shield can be used to make the sound better.\n As mentioned in Correct Pose section, we need to train the model to recognize the specific poses, performing in this case, in a more \u0026ldquo;elegant\u0026rdquo; way. If the pose can be detected not only based on the absolute positions of the points, we can have more people in the screen and play together.\n How to make the same concept possible for other kinds of instrument? For instance, to play piano by just finger drumming the table. It would be weird cool if the whole orchestra played music by posing air.\n  Reading Reflection Body movements tell a story. It\u0026rsquo;s all our gestures, motion, and dance are about. Our project, though in a simple and amateur way, is inspired by the concept - we connect body language and music. And I\u0026rsquo;m looking forward to seeing more possibilities out of the concept.\nCOCO Dataset is much more user friendly than ImageNet, with clear icons, styling illustration, and faster response. However, the biggest difference is the sorting mechanism: while ImageNet can only lookup an image based on a single tree path, COCO dataset enables user to filter the results by adding or removing different labels of things that the pictures contain. Just as Arthur said: \u0026ldquo;In order to understand a scene, each visual information has to be associated to an entity while considering the spatial information.\u0026rdquo; However, the labels, based on the things in the image, are not always completed. For example, in a picture with both elephants and trees in the background, the animal part is distinguished and labeled while the plants are not.\nThe same as ImageNet, many pictures with people can be found in the data set - and many are in private scenarios like bedroom, which might cause potential privacy and ethical problems. And the same as ImageNet, COCO claims that The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. Are there better ways of collecting data from human-related scenes?\nRelated Readings  Real-time Human Pose Estimation in the Browser with TensorFlow.js Mixing movement and machine Review of Deep Learning Algorithms for Image Semantic Segmentation COCO Dataset  "},{"idx":6,"href":"/posts/biotechnology/readings/","title":"Readings","content":" Readings Scale Matters \u0026ndash; A Fantasy of Gene Drive\nJaws (1975). Steven Spielberg. IMDB.\n\nWhat if we can modify scales of creatures with Gene Drive?\r\r\n  How we grow? The GH1 gene provides instructions for making the growth hormone protein. Growth hormone is produced in the growth-stimulating somatotropic cells of the pituitary gland, which is located at the base of the brain. Growth hormone is necessary for the normal growth of the body\u0026rsquo;s bones and tissues. The production of growth hormone is triggered when two other hormones are turned on (activated): ghrelin, which is produced in the stomach; and growth hormone releasing hormone, which is produced in a part of the brain called the hypothalamus. Ghrelin and growth hormone releasing hormone also stimulate the release of growth hormone from the pituitary gland. The release of growth hormone into the body peaks during puberty and reaches a low point at about age 55.\nCells in the liver respond to growth hormone and trigger the production of a protein called insulin-like growth factor-I (IGF-I). This protein stimulates cell growth and cell maturation (differentiation) in many different tissues, including bone. The production of IGF-I by the actions of growth hormone is a major contributor to the promotion of growth.\nGrowth hormone also plays a role in many chemical reactions (metabolic processes) in the body. By acting on specific tissues, growth hormone is involved in protein production and the breakdown (metabolism) of fats and carbohydrates.\nWhat is Teacup dog? Teacup dogs are animals that have been bred to be as small as humanly—or shall we say caninely—possible. Most dogs considered to be teacups weigh 5 pounds or less, says Los Angeles-based veterinarian Dr. Patrick Mahaney. You’ll find teacup versions of many already-small dog breeds, including teacup Poodles, teacup Pugs, and teacup Yorkies. Other popular teacup breeds include Maltese, Pomeranians, and Shih Tzus.\nTo create teacup dogs, breeders pair the so-called “runts” of the litters to make the smallest animal possible, says Dr. Cathy Meeks, a board-certified internal medicine specialist and a group medical director at BluePearl Veterinary Partners in Tampa, Florida. But sometimes the dogs selected for breeding are small because of a birth defect or other medical condition.\nDoctors say common health issues for teacup dogs include hypoglycemia, heart defects, collapsing trachea, seizures, respiratory problems, digestive problems, and blindness. The breeding practices can also lead to an increased risk for liver shunts, says Meeks. Liver shunts are often congenital birth defects in dogs that affect the liver’s ability to flush out toxins.\n20% of teacup dogs die within 3 month after birth.\nStage one We, with Gene Drive, make tiny pets everywhere in the cities.\r\rNowadays, people are already \u0026ldquo;crazy\u0026rdquo; about tiny animals that are pet-able in home. However, these teacup-sized dogs or cats have severe health problems, with unmatured organs and bones, and not well developed intelligence. With Gene Drive, however, by controlling GH1 that controls body growth only, and the genes producing hormone that controls the releasing and distributing Growth Hormone, we can thus determinedly make animals smaller by stopping their bone and muscle growth, while keeping the development and maturation of their organs.\nZ generation youngsters are already obsessed with those small and cute petty doggy and catty. With Gene Drive, however, we could make even more animals - horse, pig, eagle, or cow - pet-able and available for people to choose from, by just making them smaller. Even for traditional pet industries, tight living space for most of citizens and limited expenses people have will all persuade people from planning to raise a full-scale pet. And tiny, growth-controlled animals might be a better choice.\n A Huge Growth of Pet industry and Pet Care Industry\nThere\u0026rsquo;re more than enough reasons for people to do it. The current pet industry is 75 billion dollars (2018), and pet care industry is estimated 225 billion dollars this year (2019). And these are only with dogs and cats as \u0026ldquo;main-stream\u0026rdquo; pets. We can never imagine how the market would grow with much more \u0026ldquo;interesting\u0026rdquo; species being pets. Also, tiny pets are inevitably weaker and easier to get hurt, and we can see a even greater growth in the pet care industry. Zoologists will start to find jobs as veterinaries.\n More Homeless Abandoned Pets\n More Wild Animal Caught for Experiments\n  Stage Two We, with Gene Drive, breed scale-modified animals everywhere, from city to countryside.\r\rThen there’s no reason that we shouldn’t apply this technology to the whole ecosystem and all potentially-benefited industries. If we wanted, we could open the encyclopedia of all species on this planet, and adjust their height and weight based on our own needs: make them more beneficial to us, or less dangerous for small and weak people. The whole ecosystem will have to suffer a major change of every part of it.\nAnimal husbandry, such as breeding of pig and cow, might be benefited from it by having bigger livestock with higher weight-surface ratio. If indeterminate growers such as sharks and crocodiles, we could simply terminate their growth at any age we’d like to, and avoid potential danger. For alien invasive species, we could also make them less competitive than original breeds by making them smaller and weaker. (This might be a slower process since smaller individuals are also more likely to be less appealing to get opportunities to breed. People might need to progressively shrink their body.)\nStage Three We, with Gene Drive, make ourselves smaller and bigger.\r\rThe final stage of this carnival is the re-scaling of the scaler: human beings. After being benefited from the re-scaled world of other species, we can never hold back our desire and temptation to re-scale ourselves to take full advantage of this technology. Being high will no longer be a semi-predictable characteristic of our body, but a priceable commodity for people to differentiate themselves from others. The whole society, and its game rules, will be restructured. We’ll see giants and dwarves in our life doing different works.\n A Clear and Lawful Social Class Division\nAs we\u0026rsquo;ll have very different appearance - not in a way of aesthetics but strength, height, and capabilities, it\u0026rsquo;s hard not to have new laws and regulations for this newly formed society. Lower class people will be assigned to specific jobs and genetically modified to specific height tuned for that job. For example, coal digging or programming both doesn\u0026rsquo;t require being tall, and short people save employers\u0026rsquo; space. We\u0026rsquo;ll eventually find \u0026ldquo;perfect scale\u0026rdquo; for people to do specific jobs, and people with or without the wanted gene will be very hard to switch their jobs, to get a different pay, and make a different life. This will be the start of Genetic Discrimination.\n The Difference Will Be More and More Prominent\nGrowth gene is accumulative. So after Gene Drive, not only all the accessors will maintain the height, but also will they become taller and taller, or shorter and shorter, based on the gene from parents. The class divisions will only be solidified.\n Scale Can Be Controlled As a Whole, but Also As Parts\nWe’ll also target hormone differently to specific parts of the body.\n  The reason why I divide the whole process into three stages is that we, human, will definitely inevitably go through all the periods once the technology is accessible to us. And with three differentiated stages, we could see a clearer clue of how the progress of technology, mixed with desire and the taste if this commodity society, might affect our community in either good or bad ways. The whole story starts from a simple and leisure request from normal citizens, and it could potentially develop to a catastrophe for the whole society and ecosystem. From one stage to another, you\u0026rsquo;ll always find it reasonable, however, when looking back from the third stage to the status quo, you\u0026rsquo;ll find how crazy and pathological things we\u0026rsquo;ve done.\nReference  Which Animals Never Stop Growing? The Truth About Teacup Dogs GH1 gene Graphical representation of phenotype/gene relationship(s) associated with GH1 Why were so many prehistoric animals so big? The Biggest Trends In The Pet Industry Pet Industry Market Size \u0026amp; Ownership Statistics Moose in Canada Can evolution be predicted? For at least one important biological trait—body size—the answer appears to be yes. Pet Satisfaction among Dog Owners Isolated growth hormone deficiency Top 10 Cutest Teacup Dogs That Can Fit in Your Pocket If You Love Animals, Never Buy A Teacup Dog  Week Three We can never overestimate the power of fundamentals: microscope, a concept from 500 years ago, still serves as an essential equipment for biology experiments, plays an unreplaceable role of bringing younger generations into the field, and inspires the invention of its successor including transmission electron microscopy (TEM) as well as scanning electron microscopy (SEM), which enable modern research of much smaller scale biotechnologies. As far as I\u0026rsquo;m concerned, one fact that isolated biology apart from other science was that, for a long time, we couldn\u0026rsquo;t find one formula or law to summarize the rule of the world of life - like Newton or Gaussian\u0026rsquo;s laws. And the discovery of gene was definitely a break-through. Thanks to microscopy, we could see smaller things and dig deeper. In addition to Microscopy, are there other concepts in life science area that that also had inspired us to achieve what we achieved?\nAnother fact that we could never overestimate is how accessible knowledge and \u0026ldquo;mysterious power\u0026rdquo; can be, given the fact that we can produce a microscope with as least as $0.58. With a background of product design, this topic suddenly arose my interests. Then I searched about different kinds of (ultra-)low-cost and accessible bio equipment (for large-scale manufacturing). And here\u0026rsquo;s what I found:\n BioBits educational kits ring synthetic and molecular biology experiments into K-12 classrooms. Generic Lab Equipment from Hackteria introduces ways to DIY bio equipments including microscopy, incubator, and spectroscopy. Low-Cost Equipment for Photochemical Reactions Open-Source 3-D Platform for Low-Cost Scientific Instrument Ecosystem - a compact system for various scientific purposes.  Are there other opportunities or possibilities to make other kinds of equipment or experiment process more accessible for kids and community?\nReference  Microscopy in Biotechnology Foldscope: Origami-Based Paper Microscope  Week Two A high school student is cloning DNA at home. A former NASA scientist is live streaming experiments in his garage. A marketplace called Science Exchange is selling cloned DNA fragments that, potentially, could be used for both good or ill. All these facts are pointing to one clue: we are inevitably coming closer and closer to an era where most bio knowledge and experiments are decentralized and accessible for everyone. Community labs are awesome and form an unreplaceable piece of $370 billion bioeconomy. But we cannot ignore another fact as well: Keoni Gandall, the high-schooler, gained his fellowship in Stanford after experiments in his home, and Josiah Zayner, the former NASA scientist, worked for the government department and is still active in acknowledged academia conferences. People always say opportunities and possibilities in bio in this century are as promising as those in cyber industry from the past decades (and much much more). And after years of compiling and simplifying, coding is easy enough for everyone to learn and do. However, while \u0026ldquo;professional\u0026rdquo; programming education is now available from Youtube to summer schools, \u0026ldquo;research\u0026rdquo; labs are still only among the top tire universities. Similarly, what we should expect from the increasing community labs might be the introductory courses and accessible equipment that would bring more people into the field.\nReference  As D.I.Y. Gene Editing Gains Popularity, ‘Someone Is Going to Get Hurt’ Science Exchange TWEAKING GENES IN YOUR GARAGE: BIOHACKING BETWEEN ACTIVISM AND ENTREPRENEURSHIP On Flatbush Avenue, Seven Stories Full of Ideas Biohackers are about open-access to science, not DIY pandemics. Stop misrepresenting us  CRISPR  This is a quick note of CRISPR, a topic to familiarize and workshop in class.\n CRISPR is changing how we do science. (Phillip Sharp)\n What Clustered regularly interspaced short palindromic repeats is a family of DNA sequences found within the genomes of prokaryotic organisms such as bacteria and archaea.\nCas9 (CRISPR-associated protein 9) is an enzyme that uses CRISPR sequences as a guide to recognize and cleave specific strands of DNA that are complementary to the CRISPR sequence.\nCas9 enzymes together with CRISPR sequences form the basis of a technology known as CRISPR-Cas9 that can be used to edit genes within organisms.\nThe CRISPR-Cas system is a prokaryotic immune system that confers resistance to foreign genetic elements such as those present within plasmids and phages that provides a form of acquired immunity. RNA harboring the spacer sequence helps Cas (CRISPR-associated) proteins recognize and cut foreign pathogenic DNA. Other RNA-guided Cas proteins cut foreign RNA. CRISPR are found in approximately 50% of sequenced bacterial genomes and nearly 90% of sequenced archaea.\n How Researchers create a small piece of RNA with a short\u0026rdquo;guide\u0026rdquo; sequence that attaches (binds) to a specific target sequence of DNA in a genome. The RNA also binds to the Cas9 enzyme. As in bacteria, the modified RNA is used to recognize the DNA sequence, and the Cas9 enzyme cuts the DNA at the targeted location. Although Cas9 is the enzyme that is used most often, other enzymes (for example Cpf1) can also be used. Once the DNA is cut, researchers use the cell\u0026rsquo;s own DNA repair machinery to add or delete pieces of genetic material, or to make changes to the DNA by replacing an existing segment with a customized DNA sequence.\nCRISPR-Cas9 is proving to be an efficient and customizable alternative to other existing genome editing tools. Since the CRISPR-Cas9 system itself is capable of cutting DNA strands, CRISPRs do not need to be paired with separate cleaving enzymes as other tools do. They can also easily be matched with tailor-made “guide” RNA (gRNA) sequences designed to lead them to their DNA targets. Tens of thousands of such gRNA sequences have already been created and are available to the research community. CRISPR-Cas9 can also be used to target multiple genes simultaneously, which is another advantage that sets it apart from other gene-editing tools.\n Who First discovered in archaea (and later in bacteria) by Francisco Mojica, a scientist at the University of Alicante in Spain.\n  CRISPR genome editing allows scientists to quickly create cell and animal models, which researchers can use to accelerate research into diseases such as cancer and mental illness. In addition, CRISPR is now being developed as a rapid diagnostic. To help encourage this type of research worldwide, Feng Zhang and his team have trained thousands of researchers in the use of CRISPR genome editing technology through direct education and by sharing more than 40,000 CRISPR components with academic laboratories around the world.\nReference  CRISPR - Wikipedia Meet one of the world\u0026rsquo;s most groundbreaking scientists. ZHANG LAB Sanhana Lab Feng Zhang: The Future of Gene Editing What are genome editing and CRISPR-Cas9? QUESTIONS AND ANSWERS ABOUT CRISPR CRISPR: Gene editing and beyond CRISPR Explained Gene editing can now change an entire species \u0026ndash; forever | Jennifer Kahn What you need to know about CRISPR | Ellen Jorgensen  "},{"idx":7,"href":"/posts/performative-avatars/itseez3d/","title":"itSeez3D","content":" This week I scanned my whole body with itSeez3D app and Occipital’s Structure Sensor for capturing color and structure information. The app did a decent job for well capturing overall information including my height (a little taller) and body shape; and details from the words on my t-shirt to acne on my face.\nMy partner held the iPad and scanned me for 3 times while the first trail was a total failure and we didn\u0026rsquo;t keep it. The second scan (left two models above) went fine with some flaws occurred: for example, the texture of the top of the head didn\u0026rsquo;t match the mesh well. I\u0026rsquo;ll elaborate it below. For the third scan I tried to smile while the final facial expression turned out weird.\nThings Work Well The model looks not bad especially when people know it\u0026rsquo;s created by a tiny accessible scanner attached to an iPad. The color is accurate, the ratio and dimension preserved correctly, and the texture and shape were captured well.\nThings Don\u0026rsquo;t Work Well Internal surfaces   Although during the scan you could feel that you did scan both outer and \u0026ldquo;internal\u0026rdquo; surfaces - that are not facing the outside, through the gaps. For example, inner thigh and inner arm. However, these parts were still not captured well and in the picture above both texture and mesh of inner arm are broken.\nEdges Edge means both the edge parts of the body - head, hands, feet, etc., and the edges of the mesh. And the software is not good at meshing both. For head, although we\u0026rsquo;ve put the iPad as high as possible, the texture still looks twisted and squeezed. What\u0026rsquo;s more, it will fill all the holes by capping them with a flat surface, including the gap between my t-shirt sleeves and my arm, and the bottom of the shoes.\nStructure details The detailed structures are also not very well preserved. My ears, figures and hair are either simplified with a lot of details lost, or merged together.\nConclusion The Structure Sensor scan works well with color, meshing and texture details since it\u0026rsquo;s also helped by the iPad camera; however, is not very good at internal surfaces (faces that are noting facing outside), edges, and structure details (figures, hair, etc.) The app will re-mesh the scanned model and tends to close every opening to create a closed body.\nQuestions  When we talk about Avatar ownership, what are we talking about: Appearance, Identity, or Money?\nAvatar is capable to reflect our appearance, execute our identity, and monetize celebrities\u0026rsquo; fame. But what do people actually care about when they \u0026ldquo;fight for\u0026rdquo; non-statutory rights (in most places) of their digital representatives? Is it appearance? An app recently caught a lot of attention in China, not only because with it people can easily replace actors\u0026rsquo; faces with their own and act in as many movies as they want (like deep fake), but also because of it\u0026rsquo;s domineering user agreement - that most of people wouldn\u0026rsquo;t ever read. The original version of the agreement (that had been rewritten) granted the company the complete right of usage of image data users uploaded. However, after the thing had arisen the attention of the whole society, many people still use it and post \u0026ldquo;their\u0026rdquo; works to social media, before the company took any action. Is it Identity? Or is it Money? Should family members be eligible to make decisions of the after-demise usage of our Avatars when -\nThe Avatar is a 3D scanned model that can be programmed to do things?\nThe Avatar is so real that could pass \u0026ldquo;visual Turing test\u0026rdquo;?\nThe Avatar is formed with real biological information including our figure print and medical history?\nThe Avatar has memory of our past behaviors?  Related Readings Avatars \u0026amp; Ethics/Ownership\n Athletes \u0026amp; Video Games: Balancing Publicity Rights and the First Amendment College Football Video Games Are Done. Here’s Why. Hollywood’s Post-Biological Future: Where Actors Can Perform After Death ‘Rogue One: A Star Wars Story’ — A New Hope (For Deceased Actors) Avatar Acts: Why Online Realities Need Regulation   Some media on this page is blurred due to privacy concerns. Models scanned by Keru Wang.\n "},{"idx":8,"href":"/posts/machine-learning-for-the-arts/apple/","title":"aPPle!","content":" 🍎🥮🐟🍊🍳🍞🍗🍔🍟🥔🍚🥘💣\naPPle! is a webcam game powered by p5.js and ml5.js. In the game, various kinds of food will fly into the screen and the player need to choose only healthy food, that helps to lose weight, to eat. The player also need to dodge bombs flying into the screen from time to time. Office chair with wheels is recommended to use for playing. Here\u0026rsquo;s the LINK to play on p5 web editor.\nGame The idea comes from my struggle of keeping a diet. There\u0026rsquo;re so many kinds of food that potentially contain a great many of calories and we need to distinguish them from the healthy ones.\nThe screen is divided into three sections: Left, Center, and Right. And the food can be eaten by the user when flying to the middle of each section.\nWinning conditions The player need to eat enough healthy food to win the game while avoid eating junk or high carbohydrate food. Different food has different \u0026ldquo;bonus\u0026rdquo; or \u0026ldquo;penalty\u0026rdquo;. Since it\u0026rsquo;s a game helping me losing weight (itself involves a lot of workout when playing), the bonus will take off numbers of a preset goal and the player will win when the number is zero.\n   Food Bonus (-) Food Penalty (+)     🍎 Apple -20, trigger BoostShoot 🥘 Hotpot +20   🥮 Mooncake -10 🍗 Chicken +10   🐟 Fish -10 🥔 Potato +10   🍊 Tangerine -5 🍔 Burger +5   🍳 Egg -5 🍟 Fries +5   🍞 Bread -5 🍚 Rice +5    Also, the player need to dodge the bomb flying into the screen. The setting increases the hardness and playfulness (and workout) of the game.\nYour browser does not support the video tag.\r\rConstrains Teachable Machine is the technology behind the scene. However, this interface is incapable of training models that could recognize the position of a particular thing, in this case, my mouth. So I have to set a eating area at the middle of the screen and set 3 different body position labels. A more real game should detect the real-time position of the mouth and whether or not it\u0026rsquo;s opening solely, and calculate the \u0026ldquo;collision\u0026rdquo; of the opening area and the food.\nTeachable Machine Labels The model used in the project is trained with Teachable Machine interface. There are 7 different labels in total for the machine to learn recognize:\n Left position with mouth closed Left position with mouth open Center position with mouth closed center position with mouth open Right position with mouth closed Right position with mouth open Player in the picture  Mouth can be either open or close for 3 different body positions, and one more for no player in the frame (dodge). To make the model robust, I tried to include photos of me in different clothes, and also called my roommates to open and close their mouth in front of my webcam.\nTraining   In the end, each label has around 500 pictures to train and the training process takes around 10 minutes. While different body positions might be easy for the machine to learn, the opening and closing of the mouth, in other words, the most important part of the whole gaming experience, seemed hard for it to distinguish in this first version. I had to reduce the influencing factors that could have confused the model: making the background white and clean, stopping calling other people to be recorded, putting my head at the same height for all positions, and opening my mouth as big as possible during the recording - since machine learning models will always find \u0026ldquo;the closest label,\u0026rdquo; it shall be helpful to make different scenarios as unlike to each other as possible. The performance of the model then improved a lot.\nYour browser does not support the video tag.\r\rYour browser does not support the video tag.\r\rHowever, having all different 7 labels is solely because the model trained with teachable machine can\u0026rsquo;t locate the mouth. While Train models on poses has an interface with positioned eyes, nose and ears, but there\u0026rsquo;s no way to directly gain data from it.\nFuture Works This is my first p5.js project and my first game design. And it\u0026rsquo;s still a beta. There\u0026rsquo;re still a lot can be done but due to time and the scale of this weekly assignment that cannot be finished within the weekend. The future works include:\n Better sound effects and background music. Multiplayer system, and potential connection with the course Collective Play - two players play a same game on two computers sharing one food space. As mentioned in the constrains, current machine learning model is incapable of finding the position of the mouth thus I have to set 3 body positions and an eating area. The ideal gaming experience would be the user could decide and eat the food wherever on the screen by moving the mouth around.   Media on this page is blurred due to privacy concerns.\n "},{"idx":9,"href":"/posts/machine-learning-for-the-arts/a-glance-of-ai/","title":"A Glance of AI","content":" How to say Hi in Klingonese?  In this section a task and corresponding AI system will be designed.\n Klingonese is an artificial alien language, every word of which is well documented and can be easily looked up by Star Trek fans. But when we\u0026rsquo;re faced with real Aliens, in like 100 years, we\u0026rsquo;ll have to listen to languages we\u0026rsquo;ve never heard of before. We\u0026rsquo;ll know nothing about the vocabulary, grammar, or structure, and we\u0026rsquo;ll have no family or root languages to compare to. In this case, we could use AI as a fast learner.\nThere\u0026rsquo;ll be three stages:\n Learn\nAI could learn a language in a very different way. It doesn\u0026rsquo;t need to care about memory and searching. While we learn by words and grammars, AI can learn it as a whole through scenario-based conversations: we would have AI observe our friendly neighbor aliens and record their everyday life and communication (with their consent). Then AI would pare specific scenarios and the sentences. After a large amount of training, it would know about the language from common daily usage, to professional phrases.\n Documentation\nThen AI will learn about how we document current languages: how dictionaries are formed, how to write example sentences, and how to build records, etc. AI will help us document the alien language in a way human can easily understand and lookup.\n Teach\nFinally, AI will learn how current languages are taught, as design textbooks and class materials accordingly. And we\u0026rsquo;ll get access to those languages we could never understand before.\n  Actually, while alien language seems too imaginary, a lot of languages on this planet are endangered due to lack of documentation and proper sufficient usage. We could also use the process and system above as a remedy.\nSummary  Inputs: Recordings of conversations, articles, documents in the target language. Outputs: The language explain in known languages. Training Data: Need to be collected. Ethical Considerations: Since it\u0026rsquo;ll be a full-time recording, the privacy has to be the first thing to be considered. Data will be all made anonymous. The participants will be able to decide when to stop the recording and can exclude any piece of recording out of the database.  A People\u0026rsquo;s Guide to AI Page 23-28 (Everyday AI Activity)/36-41 (Embodying Social Algorithms).\n\rScreenshots\r↕\r\r\r\r\r"},{"idx":10,"href":"/posts/machine-learning-for-the-arts/imagenet/","title":"ImageNet","content":" The first thing that amazed me was that the project of ImageNet took full advantage of WordNet and benefited from Mechanical Turk. In this way, the data is so organized by synonym sets and, sounds crazy, human-annotated. The data can be looked up easily through treemap visualization.\nWhile each syntex, as claimed, has more than 1000 images on average to be illustrated, I still found some weak portions. With only 138 picture for People section, a lot of subdivisions, including business people and lobby boy, have no photos. The same is for Misc section. The main reason, as far as I\u0026rsquo;m concerned, is privacy. In ImageNet\u0026rsquo;s Introduction page, a whole section started with Does ImageNet own the images? Can I download the images? explains one, all photos here in the ImageNet are originally public available, and two, ImageNet only provides links and thumbnails to the actual photos for most people, like a search engine. However, there might still be potential privacy and copyright concerns. Although all the pictures are uploaded to the public space, most people might not be prepare to have their photos been used for research purposes, and been classified to be search and view easily. Given the fact that most people never read those user agreements, their consent to the photos been reused or shared, when they uploaded the photos to this services, is also doubted. ImageNet seems to have found a \u0026ldquo;perfect\u0026rdquo; way to avoid this concern coming into being, and intentionally gave up the magic power in some sensitive sections.\nTo many debating over whether citizens should sacrifice their privacy to support technology development have taken place around. Privacy is a major part of our social identity thus plays an important role of our modern humanity. Yet there\u0026rsquo;s no clue that technology and humanity are naturally opposite. We see lots of technology boosted the formation of humanity we define today, and many of them were particularly developed to protect our privacy and identity. One possible reason might be that we haven\u0026rsquo;t found the most proper way to do AI. Back to the solution itself, we shall all agree that the current solution is working and descent, yet unsatisfying. Are we only capable to analyze pixel and color? And will never understand how the world is observed and the view of it is established? I doubt that.\nTry-on Sessions Cropped screenshots of webcam image classification. Fist line (1-5): Glass Cup, Closet, Boxes, Battery, DSLR Lens. Second line (6-10): Bird on Screen, Board, Mouse, Sunglasses, Pen.\nI tried several things with a focus on the following scenarios:\n Transparent (1) Group of things (2-3) Hard for human (4-5) On screen (6) With people (7-9) Tiny target (10)  Results    Original Prediction     Glass Cup beaker, paintbrush   Closet wardrobe, closet, press   Boxes comics   Battery revolver, six-gun, six-shooter   DSLR Lens hand blower, blow dryer, hair dryer   Bird on Screen laptop, laptop computer   Board remote control   Mouse cellular telephone, mouse   Sunglasses sunglasses, dark glasses, shades   Pen syringe    "},{"idx":11,"href":"/posts/performative-avatars/i-avatar/","title":"I, Avatar","content":" Hundreds of Avatar making softwares are there for tuning our digital representations as the expectations. Specialized softwares like Adobe Fuse and MakeHuman; Sculpting softwares like Cinema4D and blender; Social media like Facebook and twitter; Mobile apps like Memoji and Kapu\u0026hellip;\nHere I used Fuse (beta) and Kapu, a newly released app by Tencent (the largest game company on the planet), created two very distinguishable Avatars of me, and analyzed the differences. While both of them are 3D software, Fuse is more for 1:1 human body making, and Kapu is more like a canvas for preschool animation.\nKAPU As the visual character making iOS app for the instant message tool QQ (targeting younger generations), Kapu is social oriented, infantilized, and silly in some perspectives. It\u0026rsquo;s not for any serious sculpting, but minimizing the making process by preset designs and limited adjustable features. Authenticity is the least to be considered in this ACG world. But honestly, the process is quite playful and pleasing, with several features worth mentioning:\n\n  \n Maniac/Interactive Motions  Every time the user switch back to the character page, (it)\u0026rsquo;ll maniacally wave Hi to you with exaggerate gestures. Unlike traditional static Avatar making software, the whole process is more like an interactive game instead of a tedious sculpting work with the embedded motion features. While waving seems childish, corresponding feedback after the user changes a particular part helps highlight the adjustment and strengthen the \u0026ldquo;gaming\u0026rdquo; experience. For example, a blink after choosing an eye type.\n     On-body Adjustment  The most important thing for a mobile Avatar app targeting kids is usability: understandable and usable for anyone who has no modeling experience. Putting clear control points on the body, instead of using sliders, Kapu is very easy and straightforward to use. This feature is also quite similar to the Liquify section in Photoshop CC.\n   Avatar in the House  As a beginner level Avatar creator, the app doesn\u0026rsquo;t offer many parameters to be changed, and the whole process is heavily based on picking from preset haircut, facial features and clothing - can be purchased in the online market. The market is also serving for another feature of the app: My Space. The Avatar can not only stand on the grey plain floor, but also be realtime rendered into a room that can be decorated and visited by the user\u0026rsquo;s friends.\n  Social Avatars Avatars are never apart from a society as humans can never. And I can see Kapu\u0026rsquo;s attempt to build digital citizenship for younger users - and once the group formed, they shall never leave. When those physical characteristics will not only be seen by the users themselves but also their QQ friends and even anonymous people, users start to hesitate, and choices disobey their interests. Tencent\u0026rsquo;s using anime and infantilized to make it easy: You can never go wrong with a cute smiling character. Actually, the community culture of QQ, started 20 years ago in 1999, plays an important role in the anonymity trend of China\u0026rsquo;s Internet, with which as the mainstream communication tool, people got used to not use self photos as profile photo, and use made-up names instead of real names.\nFuse Beta \n  \nIn contrast, Fuse is a very serious tool: detailed and cooperated with Adobe suite and Mixamo for further productions. While it\u0026rsquo;s also based on picking from some very limited preset templets, it also offers a great many of sliders for users to customize the face, body, and texture. The user can then choose from another very limited library of ugly clothes.\nYour browser does not support the video tag.\r\rAlthough there\u0026rsquo;re tons of parameters for users to adjust, templets as the base are all western faces. And I spent 30 minutes to just get it a little closer to my Asian looking.\nIdeas Quick notes of suggestions for Avatar making software.\n Aging:The models from Fuse look too mature for me, and adjusting the dimensions or length of eyes and nose manually is tedious. Since we\u0026rsquo;ve already have age recognizing models that could tell an approximate number based on a photo, is it possible for a Machine Learning based model help us generally adjust facial features? And compile the whole process into a slider: from 0 to 99 years old.  Questions  Is people\u0026rsquo;s affection to Avatars affected by how alike between them or not?\nWe see very human-alike Avatars, like the characters in SimCity, with preserved body ratio and facial features; some others are beautified intentionally, including the one mentioned above; while there\u0026rsquo;re also some quite different ones: monsters, elves, or even the cursor. What actually decides whether an Avatar will build a emotional connection with (its) owner? Should it be alike a real human? Or (it)\u0026rsquo;s owner more specifically? What role does this similarity play?\n What makes an Avatar, an Avatar?\nThere\u0026rsquo;re more and more Avatar artists, workers, servers, etc. Those are Avatars without a specified owner. Are they still an Avatar? Can we call any artificial character an Avatar? Is the Avatar of AI an Avatar?\n  Related Readings Avatars \u0026amp; Psychology/Self Representation\n Virtual Worlds Are Real Touching online funerals that gamers hold for friends they have never met Controlling vs “Being” Your Video Game Avatar When We Play Video Games, Who Are We? Aspects of the Self (Ch.7 of Life on the Screen (1995))  "},{"idx":12,"href":"/docs/","title":"Docs","content":""},{"idx":13,"href":"/","title":"Home","content":" JPL is 江沛嶺, and 江沛嶺 is Peiling Jiang. Peiling is a media arts and design student studying in NYU, Tisch School of the Arts. With a background of product design, an exchange experience in MIT Media Lab, he\u0026rsquo;s practices include industrial design and computational design, user research, programming and immersive media.\n Search by tags: IMA / Fall19  Search by categories: Reading / Project  🙆‍♂️ More information will be updated soon.\n"}];window.bookSearch={pages:pages,idx:lunr(function(){this.ref("idx");this.field("title");this.field("content");pages.forEach(this.add,this);}),}})();