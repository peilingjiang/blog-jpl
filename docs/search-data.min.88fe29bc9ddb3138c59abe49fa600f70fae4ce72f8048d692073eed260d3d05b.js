(function(){const pages=[{"idx":0,"href":"/posts/","title":"Posts","content":""},{"idx":1,"href":"/posts/machine-learning-for-the-arts/violinist/","title":"Violinist","content":" Violinist  UPDATE: The name of the project has changed to Guitarist!\n Violinist enables user to play violin (or potentially any type of instrument) by just posing as they are playing it. This simple have-a-try project is based on p5.js, ml5.js, and poseNet, and help us understand the basic use of poseNet and p5.serialControl. Here\u0026rsquo;s the link to p5 web app (p5.serialControl and Arduino required).\nposeNet() in p5.js The goal is to have the webcam and p5 web app be able to distinguish the relaxing, prepared, and playing status with poseNet(). And a few points we noticed when tackling the challenge are as follows.\nEliminate Noise Raw poseNet() output is quite noisy with more than 10 digits after the decimal point and the value changing 60 times a second. As a result, the keypoints drawn are shaking all the time and unusable for us as our calculation will be based on the position of those dots. Thus we set noiseDelta = 6, and if the horizontal or vertical position changes between frames are smaller than the value is will be regarded as noise. Moreover, we use the same strategy for minimizing the noise of angle changing, by setting sensitivity = 12 (degrees).\nCorrect Pose PoseNet() is good at abstracting the skeleton and keypoints from the body, but we didn\u0026rsquo;t find a direct way to label specific poses (e.g. running, playing violin) based on it - which might require another machine learning library. And we don\u0026rsquo;t want the speaker to beep once the bow angle changes. We then set a series of standard semi-transparent key points to help user match their pose to the performing pose. Once the pose matched, the hint areas will dim. However, this is also the constrain of the project, and if playPose() is based on the label or name of the pose, it would be much better.\nMirror Effects All the VIDEO assets drawn in p5.js sketch are flipped horizontally, unlike a natural mirror. These feature caused a huge problem for us since it\u0026rsquo;s really confusing when the figure on the screen acts the opposite way of you and you need to match your pose to the preset one. We first flip the displayed video while this will not affect the data poseNet is reading.\npush(); // Move image by the width of image to the left translate(video.width, 0); // Then scale it by -1 in the x-axis to flip the image scale(-1, 1); image(video, 0, 0, width, height); pop();  Then we setup a Point class and store all keypoints into one array. When drawing the points, renderer will only get data from getX() and getY(), and the calculation and rendering will be separated.\nclass Point { constructor(x, y, ind, part) { this.x = x; this.y = y; this.px = x; this.py = y; ... } ... getX() { // Return mirrored x return (width - this.x); } getY() { return this.y; } ... }  Physical Computing We want the buzzer to beep at different rate as the user play faster or slower. To do that, we map the velocity of angle changing to delay(x) - the faster the user is playing, the bigger the vel, and the smaller the value of x. (Please turn down the volume before playing the video.)\n\nYour browser does not support the video tag.\r\r\nFuture Works  Due to time and the scale of the project, we didn\u0026rsquo;t have enough time to further tuning the tone of buzzer. But it\u0026rsquo;ll be the first to-do on the list. Moreover, instead of playing just one note, a sequence of notes can be programmed to beep to create a real song. And mp3 shield can be used to make the sound better.\n As mentioned in Correct Pose section, we need to train the model to recognize the specific poses, performing in this case, in a more \u0026ldquo;elegant\u0026rdquo; way.\n  "},{"idx":2,"href":"/posts/biotechnology/readings/","title":"Readings","content":" CRISPR  This is a quick note of CRISPR, a topic to familiarize and workshop in class.\n CRISPR is changing how we do science. (Phillip Sharp)\n What Clustered regularly interspaced short palindromic repeats is a family of DNA sequences found within the genomes of prokaryotic organisms such as bacteria and archaea.\nCas9 (CRISPR-associated protein 9) is an enzyme that uses CRISPR sequences as a guide to recognize and cleave specific strands of DNA that are complementary to the CRISPR sequence.\nCas9 enzymes together with CRISPR sequences form the basis of a technology known as CRISPR-Cas9 that can be used to edit genes within organisms.\nThe CRISPR-Cas system is a prokaryotic immune system that confers resistance to foreign genetic elements such as those present within plasmids and phages that provides a form of acquired immunity. RNA harboring the spacer sequence helps Cas (CRISPR-associated) proteins recognize and cut foreign pathogenic DNA. Other RNA-guided Cas proteins cut foreign RNA. CRISPR are found in approximately 50% of sequenced bacterial genomes and nearly 90% of sequenced archaea.\n How Researchers create a small piece of RNA with a short\u0026rdquo;guide\u0026rdquo; sequence that attaches (binds) to a specific target sequence of DNA in a genome. The RNA also binds to the Cas9 enzyme. As in bacteria, the modified RNA is used to recognize the DNA sequence, and the Cas9 enzyme cuts the DNA at the targeted location. Although Cas9 is the enzyme that is used most often, other enzymes (for example Cpf1) can also be used. Once the DNA is cut, researchers use the cell\u0026rsquo;s own DNA repair machinery to add or delete pieces of genetic material, or to make changes to the DNA by replacing an existing segment with a customized DNA sequence.\nCRISPR-Cas9 is proving to be an efficient and customizable alternative to other existing genome editing tools. Since the CRISPR-Cas9 system itself is capable of cutting DNA strands, CRISPRs do not need to be paired with separate cleaving enzymes as other tools do. They can also easily be matched with tailor-made “guide” RNA (gRNA) sequences designed to lead them to their DNA targets. Tens of thousands of such gRNA sequences have already been created and are available to the research community. CRISPR-Cas9 can also be used to target multiple genes simultaneously, which is another advantage that sets it apart from other gene-editing tools.\n Who First discovered in archaea (and later in bacteria) by Francisco Mojica, a scientist at the University of Alicante in Spain.\n  CRISPR genome editing allows scientists to quickly create cell and animal models, which researchers can use to accelerate research into diseases such as cancer and mental illness. In addition, CRISPR is now being developed as a rapid diagnostic. To help encourage this type of research worldwide, Feng Zhang and his team have trained thousands of researchers in the use of CRISPR genome editing technology through direct education and by sharing more than 40,000 CRISPR components with academic laboratories around the world.\nReference  CRISPR - Wikipedia Meet one of the world\u0026rsquo;s most groundbreaking scientists. ZHANG LAB Sanhana Lab Feng Zhang: The Future of Gene Editing What are genome editing and CRISPR-Cas9? QUESTIONS AND ANSWERS ABOUT CRISPR CRISPR: Gene editing and beyond CRISPR Explained Gene editing can now change an entire species \u0026ndash; forever | Jennifer Kahn What you need to know about CRISPR | Ellen Jorgensen  Week Two A high school student is cloning DNA at home. A former NASA scientist is live streaming experiments in his garage. A marketplace called Science Exchange is selling cloned DNA fragments that, potentially, could be used for both good or ill. All these facts are pointing to one clue: we are inevitably coming closer and closer to an era where most bio knowledge and experiments are decentralized and accessible for everyone. Community labs are awesome and form an unreplaceable piece of $370 billion bioeconomy. But we cannot ignore another fact as well: Keoni Gandall, the high-schooler, gained his fellowship in Stanford after experiments in his home, and Josiah Zayner, the former NASA scientist, worked for the government department and is still active in acknowledged academia conferences. People always say opportunities and possibilities in bio in this century are as promising as those in cyber industry from the past decades (and much much more). And after years of compiling and simplifying, coding is easy enough for everyone to learn and do. However, while \u0026ldquo;professional\u0026rdquo; programming education is now available from Youtube to summer schools, \u0026ldquo;research\u0026rdquo; labs are still only among the top tire universities. Similarly, what we should expect from the increasing community labs might be the introductory courses and accessible equipment that would bring more people into the field.\nReference  As D.I.Y. Gene Editing Gains Popularity, ‘Someone Is Going to Get Hurt’ Science Exchange TWEAKING GENES IN YOUR GARAGE: BIOHACKING BETWEEN ACTIVISM AND ENTREPRENEURSHIP On Flatbush Avenue, Seven Stories Full of Ideas Biohackers are about open-access to science, not DIY pandemics. Stop misrepresenting us  "},{"idx":3,"href":"/posts/performative-avatars/itseez3d/","title":"itSeez3D","content":" itSeez3D This week I scanned my whole body with itSeez3D app and Occipital’s Structure Sensor for capturing color and structure information. The app did a decent job for well capturing overall information including my height (a little taller) and body shape; and details from the words on my t-shirt to acne on my face.\nMy partner held the iPad and scanned me for 3 times while the first trail was a total failure and we didn\u0026rsquo;t keep it. The second scan (left two models above) went fine with some flaws occurred: for example, the texture of the top of the head didn\u0026rsquo;t match the mesh well. I\u0026rsquo;ll elaborate it below. For the third scan I tried to smile while the final facial expression turned out weird.\nThings Work Well The model looks not bad especially when people know it\u0026rsquo;s created by a tiny accessible scanner attached to an iPad. The color is accurate, the ratio and dimension preserved correctly, and the texture and shape were captured well.\nThings Don\u0026rsquo;t Work Well Internal surfaces   Although during the scan you could feel that you did scan both outer and \u0026ldquo;internal\u0026rdquo; surfaces - that are not facing the outside, through the gaps. For example, inner thigh and inner arm. However, these parts were still not captured well and in the picture above both texture and mesh of inner arm are broken.\nEdges Edge means both the edge parts of the body - head, hands, feet, etc., and the edges of the mesh. And the software is not good at meshing both. For head, although we\u0026rsquo;ve put the iPad as high as possible, the texture still looks twisted and squeezed. What\u0026rsquo;s more, it will fill all the holes by capping them with a flat surface, including the gap between my t-shirt sleeves and my arm, and the bottom of the shoes.\nStructure details The detailed structures are also not very well preserved. My ears, figures and hair are either simplified with a lot of details lost, or merged together.\nConclusion The Structure Sensor scan works well with color, meshing and texture details since it\u0026rsquo;s also helped by the iPad camera; however, is not very good at internal surfaces (faces that are noting facing outside), edges, and structure details (figures, hair, etc.) The app will re-mesh the scanned model and tends to close every opening to create a closed body.\nQuestions  When we talk about Avatar ownership, what are we talking about: Appearance, Identity, or Money?\nAvatar is capable to reflect our appearance, execute our identity, and monetize celebrities\u0026rsquo; fame. But what do people actually care about when they \u0026ldquo;fight for\u0026rdquo; non-statutory rights (in most places) of their digital representatives? Is it appearance? An app recently caught a lot of attention in China, not only because with it people can easily replace actors\u0026rsquo; faces with their own and act in as many movies as they want (like deep fake), but also because of it\u0026rsquo;s domineering user agreement - that most of people wouldn\u0026rsquo;t ever read. The original version of the agreement (that had been rewritten) granted the company the complete right of usage of image data users uploaded. However, after the thing had arisen the attention of the whole society, many people still use it and post \u0026ldquo;their\u0026rdquo; works to social media, before the company took any action. Is it Identity? Or is it Money? Should family members be eligible to make decisions of the after-demise usage of our Avatars when -\nThe Avatar is a 3D scanned model that can be programmed to do things?\nThe Avatar is so real that could pass \u0026ldquo;visual Turing test\u0026rdquo;?\nThe Avatar is formed with real biological information including our figure print and medical history?\nThe Avatar has memory of our past behaviors?  Related Readings Avatars \u0026amp; Ethics/Ownership\n Athletes \u0026amp; Video Games: Balancing Publicity Rights and the First Amendment College Football Video Games Are Done. Here’s Why. Hollywood’s Post-Biological Future: Where Actors Can Perform After Death ‘Rogue One: A Star Wars Story’ — A New Hope (For Deceased Actors) Avatar Acts: Why Online Realities Need Regulation   Some media on this page is blurred due to privacy concerns. Models scanned by Keru Wang.\n "},{"idx":4,"href":"/posts/machine-learning-for-the-arts/apple/","title":"aPPle!","content":" aPPle! 🍎🥮🐟🍊🍳🍞🍗🍔🍟🥔🍚🥘💣\naPPle! is a webcam game powered by p5.js and ml5.js. In the game, various kinds of food will fly into the screen and the player need to choose only healthy food, that helps to lose weight, to eat. The player also need to dodge bombs flying into the screen from time to time. Office chair with wheels is recommended to use for playing. Here\u0026rsquo;s a quick link to play on p5 web editor.\nGame The idea comes from my struggle of keeping a diet. There\u0026rsquo;re so many kinds of food that potentially contain a great many of calories and we need to distinguish them from the healthy ones.\nThe screen is divided into three sections: Left, Center, and Right. And the food can be eaten by the user when flying to the middle of each section.\nWinning conditions The player need to eat enough healthy food to win the game while avoid eating junk or high carbohydrate food. Different food has different \u0026ldquo;bonus\u0026rdquo; or \u0026ldquo;penalty\u0026rdquo;. Since it\u0026rsquo;s a game helping me losing weight (itself involves a lot of workout when playing), the bonus will take off numbers of a preset goal and the player will win when the number is zero.\n   Food Bonus (-) Food Penalty (+)     🍎 Apple -20, trigger BoostShoot 🥘 Hotpot +20   🥮 Mooncake -10 🍗 Chicken +10   🐟 Fish -10 🥔 Potato +10   🍊 Tangerine -5 🍔 Burger +5   🍳 Egg -5 🍟 Fries +5   🍞 Bread -5 🍚 Rice +5    Also, the player need to dodge the bomb flying into the screen. The setting increases the hardness and playfulness (and workout) of the game.\nYour browser does not support the video tag.\r\rConstrains Teachable Machine is the technology behind the scene. However, this interface is incapable of training models that could recognize the position of a particular thing, in this case, my mouth. So I have to set a eating area at the middle of the screen and set 3 different body position labels. A more real game should detect the real-time position of the mouth and whether or not it\u0026rsquo;s opening solely, and calculate the \u0026ldquo;collision\u0026rdquo; of the opening area and the food.\nTeachable Machine Labels The model used in the project is trained with Teachable Machine interface. There are 7 different labels in total for the machine to learn recognize:\n Left position with mouth closed Left position with mouth open Center position with mouth closed center position with mouth open Right position with mouth closed Right position with mouth open Player in the picture  Mouth can be either open or close for 3 different body positions, and one more for no player in the frame (dodge). To make the model robust, I tried to include photos of me in different clothes, and also called my roommates to open and close their mouth in front of my webcam.\nTraining   In the end, each label has around 500 pictures to train and the training process takes around 10 minutes. While different body positions might be easy for the machine to learn, the opening and closing of the mouth, in other words, the most important part of the whole gaming experience, seemed hard for it to distinguish in this first version. I had to reduce the influencing factors that could have confused the model: making the background white and clean, stopping calling other people to be recorded, putting my head at the same height for all positions, and opening my mouth as big as possible during the recording - since machine learning models will always find \u0026ldquo;the closest label,\u0026rdquo; it shall be helpful to make different scenarios as unlike to each other as possible. The performance of the model then improved a lot.\nYour browser does not support the video tag.\r\rYour browser does not support the video tag.\r\rHowever, having all different 7 labels is solely because the model trained with teachable machine can\u0026rsquo;t locate the mouth. While Train models on poses has an interface with positioned eyes, nose and ears, but there\u0026rsquo;s no way to directly gain data from it.\nFuture Works This is my first p5.js project and my first game design. And it\u0026rsquo;s still a beta. There\u0026rsquo;re still a lot can be done but due to time and the scale of this weekly assignment that cannot be finished within the weekend. The future works include:\n Better sound effects and background music. Multiplayer system, and potential connection with the course Collective Play - two players play a same game on two computers sharing one food space. As mentioned in the constrains, current machine learning model is incapable of finding the position of the mouth thus I have to set 3 body positions and an eating area. The ideal gaming experience would be the user could decide and eat the food wherever on the screen by moving the mouth around.   Media on this page is blurred due to privacy concerns.\n "},{"idx":5,"href":"/posts/machine-learning-for-the-arts/a-glance-of-ai/","title":"A Glance of AI","content":" A Glance of AI How to say Hi in Klingonese?  In this section a task and corresponding AI system will be designed.\n Klingonese is an artificial alien language, every word of which is well documented and can be easily looked up by Star Trek fans. But when we\u0026rsquo;re faced with real Aliens, in like 100 years, we\u0026rsquo;ll have to listen to languages we\u0026rsquo;ve never heard of before. We\u0026rsquo;ll know nothing about the vocabulary, grammar, or structure, and we\u0026rsquo;ll have no family or root languages to compare to. In this case, we could use AI as a fast learner.\nThere\u0026rsquo;ll be three stages:\n Learn\nAI could learn a language in a very different way. It doesn\u0026rsquo;t need to care about memory and searching. While we learn by words and grammars, AI can learn it as a whole through scenario-based conversations: we would have AI observe our friendly neighbor aliens and record their everyday life and communication (with their consent). Then AI would pare specific scenarios and the sentences. After a large amount of training, it would know about the language from common daily usage, to professional phrases.\n Documentation\nThen AI will learn about how we document current languages: how dictionaries are formed, how to write example sentences, and how to build records, etc. AI will help us document the alien language in a way human can easily understand and lookup.\n Teach\nFinally, AI will learn how current languages are taught, as design textbooks and class materials accordingly. And we\u0026rsquo;ll get access to those languages we could never understand before.\n  Actually, while alien language seems too imaginary, a lot of languages on this planet are endangered due to lack of documentation and proper sufficient usage. We could also use the process and system above as a remedy.\nSummary  Inputs: Recordings of conversations, articles, documents in the target language. Outputs: The language explain in known languages. Training Data: Need to be collected. Ethical Considerations: Since it\u0026rsquo;ll be a full-time recording, the privacy has to be the first thing to be considered. Data will be all made anonymous. The participants will be able to decide when to stop the recording and can exclude any piece of recording out of the database.  A People\u0026rsquo;s Guide to AI Page 23-28 (Everyday AI Activity)/36-41 (Embodying Social Algorithms).\n\rScreenshots\r↕\r\r\r\r\r"},{"idx":6,"href":"/posts/machine-learning-for-the-arts/imagenet/","title":"ImageNet","content":" ImageNet The first thing that amazed me was that the project of ImageNet took full advantage of WordNet and benefited from Mechanical Turk. In this way, the data is so organized by synonym sets and, sounds crazy, human-annotated. The data can be looked up easily through treemap visualization.\nWhile each syntex, as claimed, has more than 1000 images on average to be illustrated, I still found some weak portions. With only 138 picture for People section, a lot of subdivisions, including business people and lobby boy, have no photos. The same is for Misc section. The main reason, as far as I\u0026rsquo;m concerned, is privacy. In ImageNet\u0026rsquo;s Introduction page, a whole section started with Does ImageNet own the images? Can I download the images? explains one, all photos here in the ImageNet are originally public available, and two, ImageNet only provides links and thumbnails to the actual photos for most people, like a search engine. However, there might still be potential privacy and copyright concerns. Although all the pictures are uploaded to the public space, most people might not be prepare to have their photos been used for research purposes, and been classified to be search and view easily. Given the fact that most people never read those user agreements, their consent to the photos been reused or shared, when they uploaded the photos to this services, is also doubted. ImageNet seems to have found a \u0026ldquo;perfect\u0026rdquo; way to avoid this concern coming into being, and intentionally gave up the magic power in some sensitive sections.\nTo many debating over whether citizens should sacrifice their privacy to support technology development have taken place around. Privacy is a major part of our social identity thus plays an important role of our modern humanity. Yet there\u0026rsquo;s no clue that technology and humanity are naturally opposite. We see lots of technology boosted the formation of humanity we define today, and many of them were particularly developed to protect our privacy and identity. One possible reason might be that we haven\u0026rsquo;t found the most proper way to do AI. Back to the solution itself, we shall all agree that the current solution is working and descent, yet unsatisfying. Are we only capable to analyze pixel and color? And will never understand how the world is observed and the view of it is established? I doubt that.\nTry-on Sessions Cropped screenshots of webcam image classification. Fist line (1-5): Glass Cup, Closet, Boxes, Battery, DSLR Lens. Second line (6-10): Bird on Screen, Board, Mouse, Sunglasses, Pen.\nI tried several things with a focus on the following scenarios:\n Transparent (1) Group of things (2-3) Hard for human (4-5) On screen (6) With people (7-9) Tiny target (10)  Results    Original Prediction     Glass Cup beaker, paintbrush   Closet wardrobe, closet, press   Boxes comics   Battery revolver, six-gun, six-shooter   DSLR Lens hand blower, blow dryer, hair dryer   Bird on Screen laptop, laptop computer   Board remote control   Mouse cellular telephone, mouse   Sunglasses sunglasses, dark glasses, shades   Pen syringe    "},{"idx":7,"href":"/posts/performative-avatars/i-avatar/","title":"I, Avatar","content":" I, Avatar Hundreds of Avatar making softwares are there for tuning our digital representations as the expectations. Specialized softwares like Adobe Fuse and MakeHuman; Sculpting softwares like Cinema4D and blender; Social media like Facebook and twitter; Mobile apps like Memoji and Kapu\u0026hellip;\nHere I used Fuse (beta) and Kapu, a newly released app by Tencent (the largest game company on the planet), created two very distinguishable Avatars of me, and analyzed the differences. While both of them are 3D software, Fuse is more for 1:1 human body making, and Kapu is more like a canvas for preschool animation.\nKAPU As the visual character making iOS app for the instant message tool QQ (targeting younger generations), Kapu is social oriented, infantilized, and silly in some perspectives. It\u0026rsquo;s not for any serious sculpting, but minimizing the making process by preset designs and limited adjustable features. Authenticity is the least to be considered in this ACG world. But honestly, the process is quite playful and pleasing, with several features worth mentioning:\n\n  \n Maniac/Interactive Motions  Every time the user switch back to the character page, (it)\u0026rsquo;ll maniacally wave Hi to you with exaggerate gestures. Unlike traditional static Avatar making software, the whole process is more like an interactive game instead of a tedious sculpting work with the embedded motion features. While waving seems childish, corresponding feedback after the user changes a particular part helps highlight the adjustment and strengthen the \u0026ldquo;gaming\u0026rdquo; experience. For example, a blink after choosing an eye type.\n     On-body Adjustment  The most important thing for a mobile Avatar app targeting kids is usability: understandable and usable for anyone who has no modeling experience. Putting clear control points on the body, instead of using sliders, Kapu is very easy and straightforward to use. This feature is also quite similar to the Liquify section in Photoshop CC.\n   Avatar in the House  As a beginner level Avatar creator, the app doesn\u0026rsquo;t offer many parameters to be changed, and the whole process is heavily based on picking from preset haircut, facial features and clothing - can be purchased in the online market. The market is also serving for another feature of the app: My Space. The Avatar can not only stand on the grey plain floor, but also be realtime rendered into a room that can be decorated and visited by the user\u0026rsquo;s friends.\n  Social Avatars Avatars are never apart from a society as humans can never. And I can see Kapu\u0026rsquo;s attempt to build digital citizenship for younger users - and once the group formed, they shall never leave. When those physical characteristics will not only be seen by the users themselves but also their QQ friends and even anonymous people, users start to hesitate, and choices disobey their interests. Tencent\u0026rsquo;s using anime and infantilized to make it easy: You can never go wrong with a cute smiling character. Actually, the community culture of QQ, started 20 years ago in 1999, plays an important role in the anonymity trend of China\u0026rsquo;s Internet, with which as the mainstream communication tool, people got used to not use self photos as profile photo, and use made-up names instead of real names.\nFuse Beta \n  \nIn contrast, Fuse is a very serious tool: detailed and cooperated with Adobe suite and Mixamo for further productions. While it\u0026rsquo;s also based on picking from some very limited preset templets, it also offers a great many of sliders for users to customize the face, body, and texture. The user can then choose from another very limited library of ugly clothes.\nYour browser does not support the video tag.\r\rAlthough there\u0026rsquo;re tons of parameters for users to adjust, templets as the base are all western faces. And I spent 30 minutes to just get it a little closer to my Asian looking.\nIdeas Quick notes of suggestions for Avatar making software.\n Aging:The models from Fuse look too mature for me, and adjusting the dimensions or length of eyes and nose manually is tedious. Since we\u0026rsquo;ve already have age recognizing models that could tell an approximate number based on a photo, is it possible for a Machine Learning based model help us generally adjust facial features? And compile the whole process into a slider: from 0 to 99 years old.  Questions  Is people\u0026rsquo;s affection to Avatars affected by how alike between them or not?\nWe see very human-alike Avatars, like the characters in SimCity, with preserved body ratio and facial features; some others are beautified intentionally, including the one mentioned above; while there\u0026rsquo;re also some quite different ones: monsters, elves, or even the cursor. What actually decides whether an Avatar will build a emotional connection with (its) owner? Should it be alike a real human? Or (it)\u0026rsquo;s owner more specifically? What role does this similarity play?\n What makes an Avatar, an Avatar?\nThere\u0026rsquo;re more and more Avatar artists, workers, servers, etc. Those are Avatars without a specified owner. Are they still an Avatar? Can we call any artificial character an Avatar? Is the Avatar of AI an Avatar?\n  Related Readings Avatars \u0026amp; Psychology/Self Representation\n Virtual Worlds Are Real Touching online funerals that gamers hold for friends they have never met Controlling vs “Being” Your Video Game Avatar When We Play Video Games, Who Are We? Aspects of the Self (Ch.7 of Life on the Screen (1995))  "},{"idx":8,"href":"/docs/","title":"Docs","content":""},{"idx":9,"href":"/","title":"Home","content":" JPL is 江沛嶺. 江沛嶺 is Peiling Jiang. This is the blog of Peiling Jiang\u0026rsquo;s works in the program of Interactive Media Arts.\n Search by tags: IMA / Fall19  Search by categories: Reading / Project  🙆‍♂️ More information will be updated soon.\n"}];window.bookSearch={pages:pages,idx:lunr(function(){this.ref("idx");this.field("title");this.field("content");pages.forEach(this.add,this);}),}})();